{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing as prep\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.3\r\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import ogb; print(ogb.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import NodePropPredDataset\n",
    "\n",
    "d = NodePropPredDataset('ogbn-arxiv', root='/datasets/ogb/ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'ogb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d2 = d2.sort_values(0).iloc[:2000]\\npartial_idx = list(set(d2[0].unique()) | set(d2[1].unique()))\\nd1 = d1.iloc[partial_idx]'"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if data =='cora':\n",
    "    d1 = pd.read_csv('cora.content', sep ='\\t', header=None)\n",
    "    d2 = pd.read_csv('cora.cites', sep ='\\t', header=None)\n",
    "elif data == 'ogb':\n",
    "    d2 = pd.DataFrame(d[0][0]['edge_index'].T)\n",
    "    d1 = pd.DataFrame(d[0][0]['node_feat'])\n",
    "\n",
    "'''d2 = d2.sort_values(0).iloc[:2000]\n",
    "partial_idx = list(set(d2[0].unique()) | set(d2[1].unique()))\n",
    "d1 = d1.iloc[partial_idx]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency matrix\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(d2.values)\n",
    "A = nx.adjacency_matrix(G).toarray()\n",
    "d2 = pd.DataFrame(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- My Implementation ---\n",
    "#d1 = (pd.read_csv('cora.content', sep ='\\t', header=None))\n",
    "\n",
    "# Label Encoder\n",
    "le = prep.LabelEncoder()\n",
    "le.fit(d1.iloc[:,-1])\n",
    "d1.iloc[:,-1] = le.transform(d1.iloc[:,-1])\n",
    "\n",
    "# Feature Matrix and Labels\n",
    "if data == 'cora':\n",
    "\n",
    "    d1 = d1.set_index(0)\n",
    "    d1 = d1.sort_index()\n",
    "    d1 = d1.reset_index()\n",
    "    labels = d1.iloc[:,-1]\n",
    "    \n",
    "else:\n",
    "    labels = d[0][1]\n",
    "\n",
    "#labels = labels[:2137].flatten()\n",
    "\n",
    "labels = torch.Tensor(labels).long()\n",
    "\n",
    "if data == 'cora':\n",
    "    columns_to_drop = [0, d1.iloc[:,-1].name]\n",
    "else:\n",
    "    columns_to_drop = [d1.iloc[:,-1].name]\n",
    "\n",
    "d1 = d1.drop(columns=columns_to_drop)\n",
    "\n",
    "X = np.array(d1)\n",
    "\n",
    "\n",
    "# Create label distibution for LPA\n",
    "\n",
    "labels_distr = np.zeros([len(labels), len(le.classes_)])\n",
    "for row in range(len(labels)):\n",
    "    labels_distr[row][labels[row]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Train/Test Indexes\n",
    "train_idx = list(d2[0].sample(frac=.9).index)\n",
    "test_idx = list(set(d2.index) - set(train_idx))\n",
    "\n",
    "'''train_A = d2.loc[train_idx, train_idx]\n",
    "train_X = d1.loc[train_idx]\n",
    "train_Y = labels[train_idx]\n",
    "\n",
    "test_A = d2.loc[test_idx, test_idx]\n",
    "test_X = d1.loc[test_idx]\n",
    "test_Y = labels[test_idx]'''\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params:\n",
    "# A is the adj matrix\n",
    "# X is the feature matrix\n",
    "class GCN_Layer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCN_Layer, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.out_feats = out_feats\n",
    "        self.weight = np.random.randn(in_feats, out_feats)\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.weight))\n",
    "\n",
    "    def forward(self, prev_output, A, prep_A=None):\n",
    "        '''\n",
    "        Propogation Rule:\n",
    "        params: pred_A - specify how to prepare A, with or without normalization\n",
    "        '''\n",
    "        prev_output = torch.Tensor(prev_output)\n",
    "        A = torch.Tensor(A)\n",
    "        \n",
    "        right_term = torch.mm(prev_output, self.weight)\n",
    "\n",
    "        # Unnormalized\n",
    "        if prep_A == None:\n",
    "            output = torch.mm(A, right_term)    \n",
    "        # Normalized with Kipf & Welling \n",
    "        elif prep_A == \"norm\":\n",
    "            I = torch.eye(A.shape[0])\n",
    "            A_hat = A + I\n",
    "            D_hat = torch.Tensor(np.diag(A_hat.sum(axis=1) ** (-1/2)))\n",
    "            output = torch.mm(D_hat, A_hat)\n",
    "            output = torch.mm(output, D_hat)\n",
    "            output = torch.mm(output, right_term)\n",
    "            \n",
    "            \n",
    "        return output\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass):\n",
    "        \"\"\"\n",
    "        Simple GCN Model\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # GCN Layers\n",
    "        self.gc1 = GCN_Layer(nfeat, nhid) \n",
    "        self.gc2 = GCN_Layer(nhid, nclass)\n",
    "        #self.gc3 = GCN_Layer(nhid-300, nclass)\n",
    "        \n",
    "    def forward(self, X, A, prep_A):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        X = F.relu(self.gc1(X, A, prep_A))\n",
    "        X = self.gc2(X, A, prep_A)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregators\n",
    "class Mean_Agg(torch.nn.Module):\n",
    "    '''\n",
    "    GraphSAGE Mean Aggregator\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Mean_Agg, self).__init__()\n",
    "        \n",
    "    def forward(self, h, A, W, activation='relu'):\n",
    "        A = torch.tensor(A)\n",
    "\n",
    "        # X: batch of nodes\n",
    "        h1 = h\n",
    "        h = torch.matmul(h.T, A) / torch.sum(A)\n",
    "        h = torch.cat((h1, h.T), 1)\n",
    "        h = torch.matmul(W, h.T.float())\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            h = F.relu(h.T)\n",
    "\n",
    "        return h\n",
    "    \n",
    "class MaxPool_Agg(torch.nn.Module):\n",
    "    '''\n",
    "    GraphSAGE Pooling Aggregator\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_feasts, out_feats):\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x, neigh):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphSAGE Models and Layers\n",
    "\n",
    "class GS_Layer(torch.nn.Module):\n",
    "    '''\n",
    "    GraphSAGE Layer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GS_Layer,self).__init__()\n",
    "        ...\n",
    "        \n",
    "    def forward(self, X, steps, A):\n",
    "        # X: batch of nodes\n",
    "        # steps: steps from node for neighborhood\n",
    "        # A: adjacency matrix to find nodes in neighborhood\n",
    "        ...\n",
    "        \n",
    "     \n",
    "# GraphSAGE\n",
    "class GS(torch.nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, agg='mean', num_samples=25, dropout=.5):\n",
    "        \"\"\"\n",
    "        GraphSAGE Model\n",
    "        \"\"\"\n",
    "        super(GS, self).__init__()\n",
    "\n",
    "        self.nfeat = nfeat\n",
    "        self.nhid = nhid\n",
    "        self.nclass = nclass\n",
    "        self.agg = agg\n",
    "        self.num_samples = num_samples\n",
    "        #self.num_layers = len(nhid) + 1\n",
    "        \n",
    "        \n",
    "        self.W = torch.randn(nfeat, 2*nfeat)\n",
    "        self.W = nn.Parameter(self.W)\n",
    "        \n",
    "        if self.agg == 'mean':\n",
    "            self.agg = Mean_Agg()\n",
    "        elif self.agg == 'maxpool':\n",
    "            self.agg = MaxPool_Agg()\n",
    "            \n",
    "        self.gc1 = GCN_Layer(nfeat, nhid) \n",
    "        self.gc2 = GCN_Layer(nhid, nclass) \n",
    "\n",
    "    def forward(self, X, A, K=1, activation='relu', prep_A='norm'):      \n",
    "        \n",
    "        # shape of H = number of nodes x number of features\n",
    "        h = torch.tensor(X)\n",
    "\n",
    "        for k in np.arange(K):\n",
    "\n",
    "            h = self.agg(h, A, self.W, activation)    \n",
    "            \n",
    "        h = F.relu(self.gc1(h.float(), A, prep_A))\n",
    "        X = self.gc2(h, A, prep_A)\n",
    "\n",
    "\n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSAGE\n"
     ]
    }
   ],
   "source": [
    "# GraphSAGE model\n",
    "def run_GS(epochs=10, Lambda=10):\n",
    "    \n",
    "    print('GraphSAGE')\n",
    "    GS_model = GS(X.shape[1], 300, len(le.classes_))\n",
    "    optimizer = torch.optim.SGD(GS_model.parameters(), lr=.1)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and Test functions\n",
    "    def train(epoch, prep_A = None):\n",
    "        t = time.time()\n",
    "        GS_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = GS_model(X, A)\n",
    "        loss = criterion(output[train_idx], labels[train_idx])\n",
    "        acc = accuracy(output[train_idx], labels[train_idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss.item()),\n",
    "              'acc_train: {:.4f}'.format(acc.item()), \n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    def test(prep_A = None):\n",
    "        GS_model.eval()\n",
    "        output = GS_model(X, A)\n",
    "        loss_test = criterion(output[test_idx], labels[test_idx])\n",
    "        acc_test = accuracy(output[test_idx], labels[test_idx])\n",
    "        print(\"Test set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_test.item()),\n",
    "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "    \n",
    "    t_total = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        train(epoch)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    # Testing\n",
    "    test()\n",
    "        \n",
    "run_GS(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPA-GCN\n",
    "\n",
    "class LPA_GCN_Layer(torch.nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, A):\n",
    "        super(LPA_GCN_Layer, self).__init__()\n",
    "        self.in_feats = in_feats\n",
    "        self.out_feats = out_feats\n",
    "        self.weight = np.random.randn(in_feats, out_feats)\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.weight))\n",
    "        A = torch.Tensor(A)\n",
    "        self.mask_A = A.clone()\n",
    "        self.mask_A = nn.Parameter(self.mask_A)\n",
    "        \n",
    "    def forward(self, X, A, Y):\n",
    "        X = torch.Tensor(X)\n",
    "        A = torch.Tensor(A)\n",
    "        Y = torch.Tensor(Y)\n",
    "        \n",
    "        right_term = torch.mm(X, self.weight)\n",
    "        # Hadamard A'\n",
    "        A = A * self.mask_A\n",
    "        # Normalize D^-1 * A'\n",
    "        A = F.normalize(A, p=1, dim=1)\n",
    "        \n",
    "        output = torch.mm(A, right_term)\n",
    "        Y_hat = torch.mm(A, Y)\n",
    "        return output, Y_hat\n",
    "    \n",
    "class GCN_LPA(torch.nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, A):\n",
    "        super(GCN_LPA, self).__init__()\n",
    "        \n",
    "        self.gcn_lpa1 = LPA_GCN_Layer(nfeat, nhid, A) \n",
    "        self.gcn_lpa2 = LPA_GCN_Layer(nhid, nclass, A) \n",
    "    \n",
    "    def forward(self, X, A, Y):\n",
    "        X, Y_hat = self.gcn_lpa1(X, A, Y)\n",
    "        X = F.relu(X)\n",
    "        X, Y_hat = self.gcn_lpa2(X, A, Y_hat)    \n",
    "        \n",
    "        return F.relu(X), F.relu(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LPA_GCN(epochs=10, Lambda=10):\n",
    "    GCN_LPA_model = GCN_LPA(X.shape[1], 300, len(le.classes_), A )\n",
    "    optimizer = torch.optim.SGD(GCN_LPA_model.parameters(), lr=.1)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train\n",
    "    for epoch in np.arange(epochs):\n",
    "        t = time.time()\n",
    "        GCN_LPA_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output, Y_hat = GCN_LPA_model(X, A, labels_distr)\n",
    "\n",
    "        loss_gcn = criterion(output[train_idx], labels[train_idx])\n",
    "        loss_lpa = criterion(Y_hat[train_idx], labels[train_idx])\n",
    "\n",
    "        acc = accuracy(output[train_idx], labels[train_idx])\n",
    "        loss_train = loss_gcn + Lambda * loss_lpa\n",
    "\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc.item()), \n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "    \n",
    "    # Test\n",
    "    GCN_LPA_model.eval()\n",
    "    output, Y_hat = GCN_LPA_model(X, A, labels_distr)\n",
    "    \n",
    "    loss_gcn = criterion(output[test_idx], labels[test_idx])\n",
    "    loss_lpa = criterion(Y_hat[test_idx], labels[test_idx])    \n",
    "    acc_test = accuracy(output[test_idx], labels[test_idx])\n",
    "    \n",
    "    loss_test = loss_gcn + Lambda * loss_lpa\n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "        \n",
    "run_LPA_GCN(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def run_GCN(epochs=10, prep_A=None):\n",
    "    print('GCN')\n",
    "    # Model and Optimizer\n",
    "    \n",
    "    # GCN takes in number of papers, number hidden layers, and number of classes\n",
    "    model = GCN(X.shape[1], 300, len(le.classes_))\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=.1)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train and Test functions\n",
    "    def train(epoch, prep_A = None):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X, A, prep_A)\n",
    "        loss = criterion(output[train_idx], labels[train_idx])\n",
    "        acc = accuracy(output[train_idx], labels[train_idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss.item()),\n",
    "              'acc_train: {:.4f}'.format(acc.item()), \n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    def test(prep_A = None):\n",
    "        model.eval()\n",
    "        output = model(X, A, prep_A)\n",
    "        loss_test = criterion(output[test_idx], labels[test_idx])\n",
    "        acc_test = accuracy(output[test_idx], labels[test_idx])\n",
    "        print(\"Test set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_test.item()),\n",
    "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "    \n",
    "    t_total = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        train(epoch, prep_A)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "    # Testing\n",
    "    test(prep_A)\n",
    "    \n",
    "run_GCN(3, 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
